{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea19c36-16f1-4af2-ada4-6e375d17f4c3",
   "metadata": {},
   "source": [
    "# Set up environment and fetch Files (Obsolete)\n",
    "this chapter was used when the data was recieved as it was ***not*** named with the UUID4 identifier and had the wrong path structure to be imported into qiime correctly. the following code has thus being marked as a comment and the metadata set merged with the mag identifier (merged_df) is provided as a result. This part being a one time use has not seen much update since the begininning, but it will have to be put up to date to generate the ready-to-use artifact containing all the mags that would be the starting point for all our qiime operations. If you uncomment the cells and want to try to run them it will create everything you need to work with the metadata down below, but will download and create a lot of garbage files you will never need again after this. Uncomment at your own risk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63ddd0e-7ef2-4c46-acab-3dbe8f193dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#set up environment\\nimport pandas as pd\\nimport qiime2 as q2\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\nimport os'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#set up environment\n",
    "import pandas as pd\n",
    "import qiime2 as q2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c66e994d-28e3-44ae-b45b-9081217460cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'! conda env list'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''! conda env list'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b71483-6ecc-442e-93ad-c5db050b7db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"%matplotlib inline\\n#!rm -r data ###unappend this line if you want to wipe clean the data folder\\ndata_dir = 'data'\\nsafe = 'permanent_dir'\\n! mkdir -p data\\n! mkdir -p permanent\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%matplotlib inline\n",
    "#!rm -r data ###unappend this line if you want to wipe clean the data folder\n",
    "data_dir = 'data'\n",
    "safe = 'permanent_dir'\n",
    "! mkdir -p data\n",
    "! mkdir -p permanent'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38613f56-c0c2-46df-8665-1104244bfb63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# fetch files from polybox\\n! wget -r -np -nH --cut-dirs=3 -R \"index.html*\" \"https://polybox.ethz.ch/index.php/s/56JaAiKdGwioBKN/download\" -O polybox_files.tar.gz\\n! tar -xzf polybox_files.tar.gz\\n\\n#unzip polybox files\\n!mv polybox_files.tar.gz polybox_files.zip\\n!unzip polybox_files.zip\\n!rm polybox_files.zip\\n\\n#untar MAGs files and store everything in data_dir\\n!tar -xzf applied_bioinformatics/Illumina_MAGs.tar.gz -C ./data\\n!tar -xzf applied_bioinformatics/PacBio_MAGs.tar.gz -C ./data\\n!mv applied_bioinformatics/merged_metadata_filtered.tsv ./data\\n!rm -r applied_bioinformatics'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# fetch files from polybox\n",
    "! wget -r -np -nH --cut-dirs=3 -R \"index.html*\" \"https://polybox.ethz.ch/index.php/s/56JaAiKdGwioBKN/download\" -O polybox_files.tar.gz\n",
    "! tar -xzf polybox_files.tar.gz\n",
    "\n",
    "#unzip polybox files\n",
    "!mv polybox_files.tar.gz polybox_files.zip\n",
    "!unzip polybox_files.zip\n",
    "!rm polybox_files.zip\n",
    "\n",
    "#untar MAGs files and store everything in data_dir\n",
    "!tar -xzf applied_bioinformatics/Illumina_MAGs.tar.gz -C ./data\n",
    "!tar -xzf applied_bioinformatics/PacBio_MAGs.tar.gz -C ./data\n",
    "!mv applied_bioinformatics/merged_metadata_filtered.tsv ./data\n",
    "!rm -r applied_bioinformatics'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c15c6-3e68-46a1-aca4-698a67a50d24",
   "metadata": {},
   "source": [
    "## Create metadata df for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485fdc46-133d-43c8-9ddb-1576e3b1e64e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from uuid import uuid4\\n# load metadata\\nmetadata_df = pd.read_csv(f\"{data_dir}/merged_metadata_filtered.tsv\", sep=\"\\t\", index_col=0)\\n\\n# rename all fasta files with UUIDs\\nfor technique in os.listdir(data_dir):\\n    tech_path = os.path.join(data_dir, technique)\\n    if not os.path.isdir(tech_path):\\n        continue\\n    for sample_id in os.listdir(tech_path):\\n        sample_path = os.path.join(tech_path, sample_id)\\n        if not os.path.isdir(sample_path):\\n            continue\\n        for file in os.listdir(sample_path):\\n            if file.endswith((\".fa\", \".fasta\")):\\n                old_path = os.path.join(sample_path, file)\\n                new_path = os.path.join(sample_path, f\"{uuid4()}.fa\")\\n                os.rename(old_path, new_path)\\n\\nrecords = []\\n\\nfor technique in os.listdir(data_dir):\\n    tech_path = os.path.join(data_dir, technique)\\n    if not os.path.isdir(tech_path):\\n        continue\\n    for sample_id in os.listdir(tech_path):\\n        sample_path = os.path.join(tech_path, sample_id)\\n        if not os.path.isdir(sample_path):\\n            continue\\n        for f in os.listdir(sample_path):\\n            if f.endswith((\".fa\", \".fasta\")):\\n                abs_path = os.path.abspath(os.path.join(sample_path, f))\\n                if sample_id in metadata_df.index:\\n                    mag_id = os.path.splitext(f)[0]  # filename without extension\\n                    records.append((sample_id, mag_id, abs_path))\\n\\n# build dataframe\\nmanifest_df = pd.DataFrame.from_records(records, columns=[\"sample-id\", \"mag-id\", \"filename\"])\\n\\n# save as MANIFEST (tab-separated, no index)\\nmanifest_path = os.path.join(data_dir, \"MANIFEST\")\\nmanifest_df.to_csv(manifest_path, sep=\",\", index=False)\\n\\nprint(f\"MANIFEST saved to: {manifest_path}\")\\nprint(manifest_df.head())\\nprint(manifest_df.columns)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from uuid import uuid4\n",
    "# load metadata\n",
    "metadata_df = pd.read_csv(f\"{data_dir}/merged_metadata_filtered.tsv\", sep=\"\\t\", index_col=0)\n",
    "\n",
    "# rename all fasta files with UUIDs\n",
    "for technique in os.listdir(data_dir):\n",
    "    tech_path = os.path.join(data_dir, technique)\n",
    "    if not os.path.isdir(tech_path):\n",
    "        continue\n",
    "    for sample_id in os.listdir(tech_path):\n",
    "        sample_path = os.path.join(tech_path, sample_id)\n",
    "        if not os.path.isdir(sample_path):\n",
    "            continue\n",
    "        for file in os.listdir(sample_path):\n",
    "            if file.endswith((\".fa\", \".fasta\")):\n",
    "                old_path = os.path.join(sample_path, file)\n",
    "                new_path = os.path.join(sample_path, f\"{uuid4()}.fa\")\n",
    "                os.rename(old_path, new_path)\n",
    "\n",
    "records = []\n",
    "\n",
    "for technique in os.listdir(data_dir):\n",
    "    tech_path = os.path.join(data_dir, technique)\n",
    "    if not os.path.isdir(tech_path):\n",
    "        continue\n",
    "    for sample_id in os.listdir(tech_path):\n",
    "        sample_path = os.path.join(tech_path, sample_id)\n",
    "        if not os.path.isdir(sample_path):\n",
    "            continue\n",
    "        for f in os.listdir(sample_path):\n",
    "            if f.endswith((\".fa\", \".fasta\")):\n",
    "                abs_path = os.path.abspath(os.path.join(sample_path, f))\n",
    "                if sample_id in metadata_df.index:\n",
    "                    mag_id = os.path.splitext(f)[0]  # filename without extension\n",
    "                    records.append((sample_id, mag_id, abs_path))\n",
    "\n",
    "# build dataframe\n",
    "manifest_df = pd.DataFrame.from_records(records, columns=[\"sample-id\", \"mag-id\", \"filename\"])\n",
    "\n",
    "# save as MANIFEST (tab-separated, no index)\n",
    "manifest_path = os.path.join(data_dir, \"MANIFEST\")\n",
    "manifest_df.to_csv(manifest_path, sep=\",\", index=False)\n",
    "\n",
    "print(f\"MANIFEST saved to: {manifest_path}\")\n",
    "print(manifest_df.head())\n",
    "print(manifest_df.columns)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2754ac20-b8b7-4a86-a319-d56d50d89a65",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EXPLORATION\n",
    "we are now ready to get to work. the following set-up cell will get all the files you need for this notebook. The metadata will be explored next to better understand how to best interrogate our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56595a4c-217b-4252-ac39-f80404a8cbd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: line 1: data_dir: command not found\n",
      "--2025-12-02 23:44:10--  https://polybox.ethz.ch/index.php/s/nMa2WaWEDft3kMr/download\n",
      "Resolving polybox.ethz.ch (polybox.ethz.ch)... 129.132.71.243\n",
      "Connecting to polybox.ethz.ch (polybox.ethz.ch)|129.132.71.243|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘data/Download.zip’\n",
      "\n",
      "data/Download.zip       [ <=>                ] 551.15K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2025-12-02 23:44:10 (58.1 MB/s) - ‘data/Download.zip’ saved [564382]\n",
      "\n",
      "Archive:  data/Download.zip\n",
      " extracting: data/01_Data_Exploration/merged_df.tsv  \n"
     ]
    }
   ],
   "source": [
    "#set up environment\n",
    "import os\n",
    "import pandas as pd\n",
    "import qiime2 as q2\n",
    "from qiime2 import Visualization\n",
    "\n",
    "# create directories for the notebook. DO NOT change\n",
    "data_dir = 'data/01_Data_Exploration'\n",
    "!data_dir = 'data/01_Data_Exploration'\n",
    "\n",
    "!mkdir -p data\n",
    "!mkdir -p $data_dir\n",
    "\n",
    "# fetches useful files for the current notebook. All files will be saved in $data_dir\n",
    "!wget 'https://polybox.ethz.ch/index.php/s/nMa2WaWEDft3kMr/download' -O data/Download.zip\n",
    "!unzip -o data/Download.zip -d data\n",
    "!rm data/Download.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88373a1b-c3d6-4bc6-8df8-af0db34f47ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'metadata_df.columns'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''metadata_df.columns'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "482125ae-f592-4d04-b610-df425bb14755",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'manifest_df.columns'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''manifest_df.columns'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6c9e3ef-013c-4648-ba3b-41e3a08b2a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"merged_df = metadata_df.merge(manifest_df, left_index=True, right_on='sample-id', how='right')\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''merged_df = metadata_df.merge(manifest_df, left_index=True, right_on='sample-id', how='right')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d6ab91-4d64-41ed-997f-37f79a318d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(f'{data_dir}/merged_df.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caa13c3c-b8be-43d2-be8d-1c7aaba95315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samp_country</th>\n",
       "      <th>category</th>\n",
       "      <th>fermented_food_type</th>\n",
       "      <th>sample-id</th>\n",
       "      <th>mag-id</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>fermented fish</td>\n",
       "      <td>Shrimp_paste_(Ka-pi)_from_yellow_shrimp</td>\n",
       "      <td>M004</td>\n",
       "      <td>17c50552-2e7b-4e25-b667-d2772874e86d</td>\n",
       "      <td>/home/jovyan/Interplanetary_Microbiome/data/Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>fermented fish</td>\n",
       "      <td>Shrimp_paste_(Ka-pi)_from_yellow_shrimp</td>\n",
       "      <td>M004</td>\n",
       "      <td>d1f9d550-67d2-4bb5-9e52-fbb071abc436</td>\n",
       "      <td>/home/jovyan/Interplanetary_Microbiome/data/Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>fermented fish</td>\n",
       "      <td>Shrimp_paste_(Ka-pi)_from_yellow_shrimp</td>\n",
       "      <td>M004</td>\n",
       "      <td>95547d8f-e2d2-4c3a-995e-4f3cec1e30fc</td>\n",
       "      <td>/home/jovyan/Interplanetary_Microbiome/data/Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>fermented fish</td>\n",
       "      <td>Shrimp_paste_(Ka-pi)_from_yellow_shrimp</td>\n",
       "      <td>M004</td>\n",
       "      <td>d132a633-c59f-4f5a-b049-144a59926482</td>\n",
       "      <td>/home/jovyan/Interplanetary_Microbiome/data/Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>fermented fish</td>\n",
       "      <td>Shrimp_paste_(Ka-pi)_from_yellow_shrimp</td>\n",
       "      <td>M004</td>\n",
       "      <td>c55dd5e6-48bb-48eb-aa0a-b8aaa6005d11</td>\n",
       "      <td>/home/jovyan/Interplanetary_Microbiome/data/Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2847</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>fermented legumes</td>\n",
       "      <td>Fermented_soybean_curd_(Tao-huu-yee)</td>\n",
       "      <td>P003</td>\n",
       "      <td>e9bbfe14-0e21-4b11-9574-79ec05e264dd</td>\n",
       "      <td>/home/jovyan/Interplanetary_Microbiome/data/Il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>fermented legumes</td>\n",
       "      <td>Fermented_soybean_curd_(Tao-huu-yee)</td>\n",
       "      <td>P003</td>\n",
       "      <td>f88751d2-c7af-4248-bdb6-67adbc8a5c78</td>\n",
       "      <td>/home/jovyan/Interplanetary_Microbiome/data/Il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>fermented legumes</td>\n",
       "      <td>Fermented_soybean_curd_(Tao-huu-yee)</td>\n",
       "      <td>P003</td>\n",
       "      <td>3a6fd3e9-d90c-4028-a24b-c96a83581958</td>\n",
       "      <td>/home/jovyan/Interplanetary_Microbiome/data/Il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>fermented legumes</td>\n",
       "      <td>Fermented_soybean_curd_(Tao-huu-yee)</td>\n",
       "      <td>P003</td>\n",
       "      <td>88572f98-8cba-45ff-af14-34eb13cdaf32</td>\n",
       "      <td>/home/jovyan/Interplanetary_Microbiome/data/Il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2851</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>fermented legumes</td>\n",
       "      <td>Fermented_soybean_curd_(Tao-huu-yee)</td>\n",
       "      <td>P003</td>\n",
       "      <td>b08fe61c-eeef-4de6-b4d7-01b341ad4b7e</td>\n",
       "      <td>/home/jovyan/Interplanetary_Microbiome/data/Il...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2852 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     samp_country           category                      fermented_food_type  \\\n",
       "0        Thailand     fermented fish  Shrimp_paste_(Ka-pi)_from_yellow_shrimp   \n",
       "1        Thailand     fermented fish  Shrimp_paste_(Ka-pi)_from_yellow_shrimp   \n",
       "2        Thailand     fermented fish  Shrimp_paste_(Ka-pi)_from_yellow_shrimp   \n",
       "3        Thailand     fermented fish  Shrimp_paste_(Ka-pi)_from_yellow_shrimp   \n",
       "4        Thailand     fermented fish  Shrimp_paste_(Ka-pi)_from_yellow_shrimp   \n",
       "...           ...                ...                                      ...   \n",
       "2847     Thailand  fermented legumes     Fermented_soybean_curd_(Tao-huu-yee)   \n",
       "2848     Thailand  fermented legumes     Fermented_soybean_curd_(Tao-huu-yee)   \n",
       "2849     Thailand  fermented legumes     Fermented_soybean_curd_(Tao-huu-yee)   \n",
       "2850     Thailand  fermented legumes     Fermented_soybean_curd_(Tao-huu-yee)   \n",
       "2851     Thailand  fermented legumes     Fermented_soybean_curd_(Tao-huu-yee)   \n",
       "\n",
       "     sample-id                                mag-id  \\\n",
       "0         M004  17c50552-2e7b-4e25-b667-d2772874e86d   \n",
       "1         M004  d1f9d550-67d2-4bb5-9e52-fbb071abc436   \n",
       "2         M004  95547d8f-e2d2-4c3a-995e-4f3cec1e30fc   \n",
       "3         M004  d132a633-c59f-4f5a-b049-144a59926482   \n",
       "4         M004  c55dd5e6-48bb-48eb-aa0a-b8aaa6005d11   \n",
       "...        ...                                   ...   \n",
       "2847      P003  e9bbfe14-0e21-4b11-9574-79ec05e264dd   \n",
       "2848      P003  f88751d2-c7af-4248-bdb6-67adbc8a5c78   \n",
       "2849      P003  3a6fd3e9-d90c-4028-a24b-c96a83581958   \n",
       "2850      P003  88572f98-8cba-45ff-af14-34eb13cdaf32   \n",
       "2851      P003  b08fe61c-eeef-4de6-b4d7-01b341ad4b7e   \n",
       "\n",
       "                                               filename  \n",
       "0     /home/jovyan/Interplanetary_Microbiome/data/Pa...  \n",
       "1     /home/jovyan/Interplanetary_Microbiome/data/Pa...  \n",
       "2     /home/jovyan/Interplanetary_Microbiome/data/Pa...  \n",
       "3     /home/jovyan/Interplanetary_Microbiome/data/Pa...  \n",
       "4     /home/jovyan/Interplanetary_Microbiome/data/Pa...  \n",
       "...                                                 ...  \n",
       "2847  /home/jovyan/Interplanetary_Microbiome/data/Il...  \n",
       "2848  /home/jovyan/Interplanetary_Microbiome/data/Il...  \n",
       "2849  /home/jovyan/Interplanetary_Microbiome/data/Il...  \n",
       "2850  /home/jovyan/Interplanetary_Microbiome/data/Il...  \n",
       "2851  /home/jovyan/Interplanetary_Microbiome/data/Il...  \n",
       "\n",
       "[2852 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a3590eb-dc39-4408-81c9-cb6d7edeefa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Thailand', 'Switzerland', 'Benin', 'Slovenia', 'Germany', 'Laos'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['samp_country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56cd41df-29e3-44f8-9148-5879263295f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "samp_country\n",
       "Benin           400\n",
       "Germany          20\n",
       "Laos           1136\n",
       "Slovenia         38\n",
       "Switzerland      16\n",
       "Thailand       1242\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.groupby('samp_country').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87b25d-f824-4350-b548-2de79186d541",
   "metadata": {},
   "source": [
    "Our samples were sampled in 6 different countries: Benin, Germany, Laos, Slovenia, Switzerland, and Thailand. Most samples come from Thailand and Laos (621 and 568, respectively), while the least number of samples were taken in Switzerland."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a4eb5-fa2d-430f-a556-96f1b1386f44",
   "metadata": {},
   "source": [
    "**Note: the commented out code has a long output since it calculates for all files, so we only give averages for all files of the following calculations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f2854-26d7-4bc2-af03-3483c42c1cb5",
   "metadata": {},
   "source": [
    "#### Calculating GC content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "336aec3b-5adb-4f1a-9a3b-f89984eafd7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def calculate_gc_content(sequence):\\n    g = sequence.count(\\'G\\')\\n    c = sequence.count(\\'C\\')\\n    gc_content = (g + c) / len(sequence) * 100 if len(sequence) > 0 else 0\\n    return gc_content\\n\\n\\ndef analyze_gc_content(fasta_file):\\n    gc_contents = []\\n    with open(fasta_file, \\'r\\') as file:\\n        sequence = \"\"\\n        for line in file:\\n            line = line.strip()\\n            if line.startswith(\">\"):  # Header line (ignore it)\\n                if sequence:  # If there was a previous sequence, process it\\n                    gc_contents.append(calculate_gc_content(sequence))\\n                sequence = \"\"  # Reset sequence for the next record\\n            else:\\n                sequence += line  # Append sequence data\\n        if sequence:\\n            gc_contents.append(calculate_gc_content(sequence))\\n    return gc_contents\\n\\n# Function that walk through directories and process FASTA files\\ndef process_directory(base_directory):\\n    gc_results = {}\\n\\n    for root, dirs, files in os.walk(base_directory):\\n        for file in files:\\n            if file.endswith(\".fasta\") or file.endswith(\".fa\"):\\n                fasta_file_path = os.path.join(root, file)\\n                gc_contents = analyze_gc_content(fasta_file_path)\\n                # Save results -> file path as the key and GC contents as the value\\n                gc_results[fasta_file_path] = gc_contents\\n\\n    return gc_results\\n\\n\\nillumina_folder = \\'data/Illumina_MAGs\\'\\npacbio_folder = \\'data/PacBio_MAGs\\'\\n\\n\\nillumina_gc = process_directory(illumina_folder)\\npacbio_gc = process_directory(pacbio_folder)\\n\\n\\nprint(\"Illumina GC Content:\")\\nfor file, gc in illumina_gc.items():\\n    print(f\"{file}: Average GC content = {sum(gc)/len(gc):.2f}%\")\\n\\nprint(\"\\nPacBio GC Content:\")\\nfor file, gc in pacbio_gc.items():\\n    print(f\"{file}: Average GC content = {sum(gc)/len(gc):.2f}%\")'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def calculate_gc_content(sequence):\n",
    "    g = sequence.count('G')\n",
    "    c = sequence.count('C')\n",
    "    gc_content = (g + c) / len(sequence) * 100 if len(sequence) > 0 else 0\n",
    "    return gc_content\n",
    "\n",
    "\n",
    "def analyze_gc_content(fasta_file):\n",
    "    gc_contents = []\n",
    "    with open(fasta_file, 'r') as file:\n",
    "        sequence = \"\"\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):  # Header line (ignore it)\n",
    "                if sequence:  # If there was a previous sequence, process it\n",
    "                    gc_contents.append(calculate_gc_content(sequence))\n",
    "                sequence = \"\"  # Reset sequence for the next record\n",
    "            else:\n",
    "                sequence += line  # Append sequence data\n",
    "        if sequence:\n",
    "            gc_contents.append(calculate_gc_content(sequence))\n",
    "    return gc_contents\n",
    "\n",
    "# Function that walk through directories and process FASTA files\n",
    "def process_directory(base_directory):\n",
    "    gc_results = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".fasta\") or file.endswith(\".fa\"):\n",
    "                fasta_file_path = os.path.join(root, file)\n",
    "                gc_contents = analyze_gc_content(fasta_file_path)\n",
    "                # Save results -> file path as the key and GC contents as the value\n",
    "                gc_results[fasta_file_path] = gc_contents\n",
    "\n",
    "    return gc_results\n",
    "\n",
    "\n",
    "illumina_folder = 'data/Illumina_MAGs'\n",
    "pacbio_folder = 'data/PacBio_MAGs'\n",
    "\n",
    "\n",
    "illumina_gc = process_directory(illumina_folder)\n",
    "pacbio_gc = process_directory(pacbio_folder)\n",
    "\n",
    "\n",
    "print(\"Illumina GC Content:\")\n",
    "for file, gc in illumina_gc.items():\n",
    "    print(f\"{file}: Average GC content = {sum(gc)/len(gc):.2f}%\")\n",
    "\n",
    "print(\"\\nPacBio GC Content:\")\n",
    "for file, gc in pacbio_gc.items():\n",
    "    print(f\"{file}: Average GC content = {sum(gc)/len(gc):.2f}%\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4db310f-0c0d-4bbc-9117-9146f4f84a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average GC content for Illumina: 0.00%\n",
      "Average GC content for PacBio: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "\n",
    "def calculate_gc_content(sequence):\n",
    "    g = sequence.count('G')\n",
    "    c = sequence.count('C')\n",
    "    gc_content = (g + c) / len(sequence) * 100 if len(sequence) > 0 else 0\n",
    "    return gc_content\n",
    "\n",
    "def analyze_gc_content(fasta_file):\n",
    "    gc_contents = []\n",
    "    with open(fasta_file, 'r') as file:\n",
    "        sequence = \"\"\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):  # Header line (ignore it)\n",
    "                if sequence:  # If there was a previous sequence, process it\n",
    "                    gc_contents.append(calculate_gc_content(sequence))\n",
    "                sequence = \"\"  # Reset sequence for the next record\n",
    "            else:\n",
    "                sequence += line  # Append sequence data\n",
    "        if sequence: \n",
    "            gc_contents.append(calculate_gc_content(sequence))\n",
    "    return gc_contents\n",
    "\n",
    "# Function that walk through directories and process FASTA files\n",
    "def process_directory(base_directory):\n",
    "    all_gc_contents = []  # List to store all GC content values across all files\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".fasta\") or file.endswith(\".fa\"):\n",
    "                fasta_file_path = os.path.join(root, file)\n",
    "                gc_contents = analyze_gc_content(fasta_file_path)\n",
    "                all_gc_contents.extend(gc_contents)  # Add the GC content from this file to the list\n",
    "    return all_gc_contents\n",
    "\n",
    "\n",
    "illumina_folder = 'data/Illumina_MAGs'\n",
    "pacbio_folder = 'data/PacBio_MAGs'\n",
    "\n",
    "\n",
    "illumina_gc = process_directory(illumina_folder)\n",
    "pacbio_gc = process_directory(pacbio_folder)\n",
    "\n",
    "\n",
    "def calculate_average_gc(gc_contents):\n",
    "    return sum(gc_contents) / len(gc_contents) if gc_contents else 0\n",
    "\n",
    "\n",
    "print(f\"Average GC content for Illumina: {calculate_average_gc(illumina_gc):.2f}%\")\n",
    "print(f\"Average GC content for PacBio: {calculate_average_gc(pacbio_gc):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee43c51-2cd6-4e8b-9069-910aa137aca5",
   "metadata": {},
   "source": [
    "**Both technologies recovered genomes with very similar base compositions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ce24d-d0b1-45c3-bbed-60c6a01db98a",
   "metadata": {},
   "source": [
    "#### Calculating N50 and L50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23d68a93-f481-4830-8bf9-bdf48a105250",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Function to calculate N50 and L50\\ndef calculate_n50_l50(contig_lengths):\\n    \"\"\"Calculate the N50 and L50 of the contig lengths.\"\"\"\\n    contig_lengths.sort(reverse=True)  # Sort contig lengths in descending order\\n    total_length = sum(contig_lengths)\\n    half_total_length = total_length / 2\\n\\n    cumulative_length = 0\\n    n50 = 0\\n    l50 = 0\\n    for length in contig_lengths:\\n        cumulative_length += length\\n        l50 += 1\\n        if cumulative_length >= half_total_length and n50 == 0:\\n            n50 = length\\n    \\n    return n50, l50\\n\\n# Function to get the lengths of sequences from a FASTA file\\ndef get_contig_lengths(fasta_file):\\n    \"\"\"Reads a FASTA file and returns a list of contig lengths.\"\"\"\\n    contig_lengths = []\\n    with open(fasta_file, \\'r\\') as file:\\n        sequence = \"\"\\n        for line in file:\\n            line = line.strip()\\n            if line.startswith(\">\"):  # Header line (ignore it)\\n                if sequence:  # If there was a previous sequence, process it\\n                    contig_lengths.append(len(sequence))\\n                sequence = \"\"  # Reset sequence for the next record\\n            else:\\n                sequence += line  # Appends sequence data\\n        if sequence:  # Process the last sequence\\n            contig_lengths.append(len(sequence))\\n    return contig_lengths\\n\\n# Function that walk through directories and process FASTA files\\ndef process_directory(base_directory):\\n    n50_results = {}\\n\\n    for root, dirs, files in os.walk(base_directory):\\n        for file in files:\\n            if file.endswith(\".fasta\") or file.endswith(\".fa\"):\\n                fasta_file_path = os.path.join(root, file)\\n                \\n                # Get contig lengths from the file\\n                contig_lengths = get_contig_lengths(fasta_file_path)\\n                \\n                # If there are valid contigs, calculates N50 and L50\\n                if contig_lengths:\\n                    n50, l50 = calculate_n50_l50(contig_lengths)\\n                    n50_results[fasta_file_path] = (n50, l50)\\n\\n    return n50_results\\n\\nillumina_folder = \\'data/Illumina_MAGs\\'\\npacbio_folder = \\'data/PacBio_MAGs\\'\\n\\n# Process the directories and calculate N50 and L50\\nillumina_n50 = process_directory(illumina_folder)\\npacbio_n50 = process_directory(pacbio_folder)\\n\\nprint(\"Illumina N50 and L50:\")\\nfor file, (n50, l50) in illumina_n50.items():\\n    print(f\"{file}: N50 = {n50}, L50 = {l50}\")\\n\\nprint(\"\\nPacBio N50 and L50:\")\\nfor file, (n50, l50) in pacbio_n50.items():\\n    print(f\"{file}: N50 = {n50}, L50 = {l50}\")'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Function to calculate N50 and L50\n",
    "def calculate_n50_l50(contig_lengths):\n",
    "    \"\"\"Calculate the N50 and L50 of the contig lengths.\"\"\"\n",
    "    contig_lengths.sort(reverse=True)  # Sort contig lengths in descending order\n",
    "    total_length = sum(contig_lengths)\n",
    "    half_total_length = total_length / 2\n",
    "\n",
    "    cumulative_length = 0\n",
    "    n50 = 0\n",
    "    l50 = 0\n",
    "    for length in contig_lengths:\n",
    "        cumulative_length += length\n",
    "        l50 += 1\n",
    "        if cumulative_length >= half_total_length and n50 == 0:\n",
    "            n50 = length\n",
    "    \n",
    "    return n50, l50\n",
    "\n",
    "# Function to get the lengths of sequences from a FASTA file\n",
    "def get_contig_lengths(fasta_file):\n",
    "    \"\"\"Reads a FASTA file and returns a list of contig lengths.\"\"\"\n",
    "    contig_lengths = []\n",
    "    with open(fasta_file, 'r') as file:\n",
    "        sequence = \"\"\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):  # Header line (ignore it)\n",
    "                if sequence:  # If there was a previous sequence, process it\n",
    "                    contig_lengths.append(len(sequence))\n",
    "                sequence = \"\"  # Reset sequence for the next record\n",
    "            else:\n",
    "                sequence += line  # Appends sequence data\n",
    "        if sequence:  # Process the last sequence\n",
    "            contig_lengths.append(len(sequence))\n",
    "    return contig_lengths\n",
    "\n",
    "# Function that walk through directories and process FASTA files\n",
    "def process_directory(base_directory):\n",
    "    n50_results = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".fasta\") or file.endswith(\".fa\"):\n",
    "                fasta_file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Get contig lengths from the file\n",
    "                contig_lengths = get_contig_lengths(fasta_file_path)\n",
    "                \n",
    "                # If there are valid contigs, calculates N50 and L50\n",
    "                if contig_lengths:\n",
    "                    n50, l50 = calculate_n50_l50(contig_lengths)\n",
    "                    n50_results[fasta_file_path] = (n50, l50)\n",
    "\n",
    "    return n50_results\n",
    "\n",
    "illumina_folder = 'data/Illumina_MAGs'\n",
    "pacbio_folder = 'data/PacBio_MAGs'\n",
    "\n",
    "# Process the directories and calculate N50 and L50\n",
    "illumina_n50 = process_directory(illumina_folder)\n",
    "pacbio_n50 = process_directory(pacbio_folder)\n",
    "\n",
    "print(\"Illumina N50 and L50:\")\n",
    "for file, (n50, l50) in illumina_n50.items():\n",
    "    print(f\"{file}: N50 = {n50}, L50 = {l50}\")\n",
    "\n",
    "print(\"\\nPacBio N50 and L50:\")\n",
    "for file, (n50, l50) in pacbio_n50.items():\n",
    "    print(f\"{file}: N50 = {n50}, L50 = {l50}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0871dad9-2f87-47a0-938b-3689365efee8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illumina N50: 0\n",
      "Illumina L50: 0\n",
      "PacBio N50: 0\n",
      "PacBio L50: 0\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate N50 and L50\n",
    "def calculate_n50_l50(contig_lengths):\n",
    "    \"\"\"Calculate the N50 and L50 of the contig lengths.\"\"\"\n",
    "    contig_lengths.sort(reverse=True)  # Sort contig lengths in descending order\n",
    "    total_length = sum(contig_lengths)\n",
    "    half_total_length = total_length / 2\n",
    "\n",
    "    cumulative_length = 0\n",
    "    n50 = 0\n",
    "    l50 = 0\n",
    "    for length in contig_lengths:\n",
    "        cumulative_length += length\n",
    "        l50 += 1\n",
    "        if cumulative_length >= half_total_length and n50 == 0:\n",
    "            n50 = length\n",
    "    \n",
    "    return n50, l50\n",
    "\n",
    "# Function to get the lengths of sequences from a FASTA file\n",
    "def get_contig_lengths(fasta_file):\n",
    "    \"\"\"Reads a FASTA file and returns a list of contig lengths.\"\"\"\n",
    "    contig_lengths = []\n",
    "    with open(fasta_file, 'r') as file:\n",
    "        sequence = \"\"\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):  # Header line (ignore it)\n",
    "                if sequence:  # If there was a previous sequence, process it\n",
    "                    contig_lengths.append(len(sequence))\n",
    "                sequence = \"\"  # Reset sequence for the next record\n",
    "            else:\n",
    "                sequence += line  # Append sequence data\n",
    "        if sequence:  # Process the last sequence\n",
    "            contig_lengths.append(len(sequence))\n",
    "    return contig_lengths\n",
    "\n",
    "# Function that walk through directories and process FASTA files\n",
    "def process_directory(base_directory):\n",
    "    all_contig_lengths = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".fasta\") or file.endswith(\".fa\"):\n",
    "                fasta_file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Getting contig lengths from the file\n",
    "                contig_lengths = get_contig_lengths(fasta_file_path)\n",
    "                \n",
    "                # Adding contig lengths to the overall list\n",
    "                if contig_lengths:\n",
    "                    all_contig_lengths.extend(contig_lengths)\n",
    "\n",
    "    return all_contig_lengths\n",
    "\n",
    "illumina_folder = 'data/Illumina_MAGs'\n",
    "pacbio_folder = 'data/PacBio_MAGs'\n",
    "\n",
    "# Processing the directories and getting all contig lengths\n",
    "illumina_contig_lengths = process_directory(illumina_folder)\n",
    "pacbio_contig_lengths = process_directory(pacbio_folder)\n",
    "\n",
    "# Calculating N50 and L50 for Illumina\n",
    "if illumina_contig_lengths:\n",
    "    illumina_n50, illumina_l50 = calculate_n50_l50(illumina_contig_lengths)\n",
    "else:\n",
    "    illumina_n50, illumina_l50 = 0, 0\n",
    "\n",
    "# Calculating N50 and L50 for PacBio\n",
    "if pacbio_contig_lengths:\n",
    "    pacbio_n50, pacbio_l50 = calculate_n50_l50(pacbio_contig_lengths)\n",
    "else:\n",
    "    pacbio_n50, pacbio_l50 = 0, 0\n",
    "\n",
    "\n",
    "print(f\"Illumina N50: {illumina_n50}\")\n",
    "print(f\"Illumina L50: {illumina_l50}\")\n",
    "\n",
    "print(f\"PacBio N50: {pacbio_n50}\")\n",
    "print(f\"PacBio L50: {pacbio_l50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f7027-3333-4be2-922b-ce3b7a01b957",
   "metadata": {},
   "source": [
    "**PacBio assemblies are more contiguous, with a 15-fold higher N50 and a 13-fold lower L50 compared to Illumina assemblies.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7ca37-f89c-4345-b55e-fed6ed566ae6",
   "metadata": {},
   "source": [
    "# Genome Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01433cc5-0383-4163-af05-8fc7a87441c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up environment\n",
    "import pandas as pd\n",
    "import qiime2 as q2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gzip, io\n",
    "from qiime2 import Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a4ee3f0-1367-4822-9766-14089865eb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-02 23:44:12--  https://polybox.ethz.ch/index.php/s/56JaAiKdGwioBKN/download\n",
      "Resolving polybox.ethz.ch (polybox.ethz.ch)... 129.132.71.243\n",
      "Connecting to polybox.ethz.ch (polybox.ethz.ch)|129.132.71.243|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘data/01_Data_Exploration/polybox_files.zip’\n",
      "\n",
      "data/01_Data_Explor     [       <=>          ] 492.28M   409MB/s    in 1.2s    \n",
      "\n",
      "2025-12-02 23:44:13 (409 MB/s) - ‘data/01_Data_Exploration/polybox_files.zip’ saved [516188455]\n",
      "\n",
      "Archive:  data/01_Data_Exploration/polybox_files.zip\n",
      "   creating: data/01_Data_Exploration/applied_bioinformatics/\n",
      " extracting: data/01_Data_Exploration/applied_bioinformatics/.DS_Store  \n",
      " extracting: data/01_Data_Exploration/applied_bioinformatics/Illumina_MAGs.tar.gz  \n",
      " extracting: data/01_Data_Exploration/applied_bioinformatics/PacBio_MAGs.tar.gz  \n",
      " extracting: data/01_Data_Exploration/applied_bioinformatics/merged_metadata_filtered.tsv  \n"
     ]
    }
   ],
   "source": [
    "#### fetch files from polybox\n",
    "! wget -O $data_dir/polybox_files.zip \"https://polybox.ethz.ch/index.php/s/56JaAiKdGwioBKN/download\"\n",
    "\n",
    "# unzip polybox files\n",
    "! unzip -o $data_dir/polybox_files.zip -d $data_dir\n",
    "! rm $data_dir/polybox_files.zip\n",
    "\n",
    "#untar MAGs files and store everything in data_dir\n",
    "!tar -xzf $data_dir/applied_bioinformatics/Illumina_MAGs.tar.gz -C $data_dir\n",
    "!tar -xzf $data_dir/applied_bioinformatics/PacBio_MAGs.tar.gz -C $data_dir\n",
    "!mv $data_dir/applied_bioinformatics/merged_metadata_filtered.tsv $data_dir\n",
    "!rm -r $data_dir/applied_bioinformatics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d82bd6b-df81-468f-8805-7fc943f2e28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MANIFEST saved to: data/01_Data_Exploration/MANIFEST\n",
      "  sample-id                                mag-id  \\\n",
      "0      M004  9d47f527-5408-4d04-9020-fdfbbe31d080   \n",
      "1      M004  88baf06d-d00b-4c05-a27f-2cc006906d54   \n",
      "2      M004  27d504ba-3503-47ba-b067-b933eba193d0   \n",
      "3      M004  c3be7c77-51ae-4452-afb6-e7829ba1c4b6   \n",
      "4      M004  c21cdfa3-3621-48eb-adf4-7a116770dff6   \n",
      "\n",
      "                                            filename  \n",
      "0  /home/jovyan/REPORT/data/01_Data_Exploration/P...  \n",
      "1  /home/jovyan/REPORT/data/01_Data_Exploration/P...  \n",
      "2  /home/jovyan/REPORT/data/01_Data_Exploration/P...  \n",
      "3  /home/jovyan/REPORT/data/01_Data_Exploration/P...  \n",
      "4  /home/jovyan/REPORT/data/01_Data_Exploration/P...  \n",
      "Index(['sample-id', 'mag-id', 'filename'], dtype='object')\n",
      "Checking short data:\n",
      "  Max size: 105229916 bp\n",
      "  Min size: 200090 bp\n",
      "  Mean size: 1475088.4283088236 bp\n",
      "  Number of sequences: 1088\n",
      "Checking long data:\n",
      "  Max size: 26029257 bp\n",
      "  Min size: 200069 bp\n",
      "  Mean size: 961954.9648526077 bp\n",
      "  Number of sequences: 1764\n",
      "Wrote genome_sizes_metadata.tsv\n",
      "/opt/conda/lib/python3.10/site-packages/unifrac/__init__.py:9: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "\u001b[32mSaved Visualization to: data/01_Data_Exploration/genome_sizes_table.qzv\u001b[0m\n",
      "\u001b[0m\u001b[?25h          n     mean  median     q25      q75     min        max\n",
      "_tech                                                           \n",
      "long   1764   961.95  431.53  272.66   966.13  200.07   26029.26\n",
      "short  1088  1475.09  903.05  411.25  1744.61  200.09  105229.92\n"
     ]
    }
   ],
   "source": [
    "# Create metadata df for exploration\n",
    "from uuid import uuid4\n",
    "\n",
    "# load metadata (sample-level metadata table, indexed by sample-id)\n",
    "metadata_df = pd.read_csv(f\"{data_dir}/merged_metadata_filtered.tsv\", sep=\"\\t\", index_col=0)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Rename all FASTA files with UUIDs\n",
    "#    - This avoids collisions if different samples had identical filenames.\n",
    "#    - We traverse all subdirectories under data_dir and rename *.fa / *.fasta.\n",
    "# ---------------------------------------------------------------------\n",
    "for technique in os.listdir(data_dir):\n",
    "    tech_path = os.path.join(data_dir, technique)\n",
    "    if not os.path.isdir(tech_path):\n",
    "        continue\n",
    "    for sample_id in os.listdir(tech_path):\n",
    "        sample_path = os.path.join(tech_path, sample_id)\n",
    "        if not os.path.isdir(sample_path):\n",
    "            continue\n",
    "        for file in os.listdir(sample_path):\n",
    "            if file.endswith((\".fa\", \".fasta\")):\n",
    "                old_path = os.path.join(sample_path, file)\n",
    "                # use a UUID as a new filename (still with .fa extension)\n",
    "                new_path = os.path.join(sample_path, f\"{uuid4()}.fa\")\n",
    "                os.rename(old_path, new_path)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Build a manifest-like table linking sample-id, MAG ID and filename\n",
    "#    - We walk the directory tree again, collect all FASTA files,\n",
    "#      and only keep entries where sample_id exists in metadata_df.\n",
    "# ---------------------------------------------------------------------\n",
    "records = []\n",
    "\n",
    "for technique in os.listdir(data_dir):\n",
    "    tech_path = os.path.join(data_dir, technique)\n",
    "    if not os.path.isdir(tech_path):\n",
    "        continue\n",
    "    for sample_id in os.listdir(tech_path):\n",
    "        sample_path = os.path.join(tech_path, sample_id)\n",
    "        if not os.path.isdir(sample_path):\n",
    "            continue\n",
    "        for f in os.listdir(sample_path):\n",
    "            if f.endswith((\".fa\", \".fasta\")):\n",
    "                abs_path = os.path.abspath(os.path.join(sample_path, f))\n",
    "                # only include sample IDs that are present in the metadata\n",
    "                if sample_id in metadata_df.index:\n",
    "                    mag_id = os.path.splitext(f)[0]  # filename without extension\n",
    "                    records.append((sample_id, mag_id, abs_path))\n",
    "\n",
    "# build dataframe\n",
    "manifest_df = pd.DataFrame.from_records(records, columns=[\"sample-id\", \"mag-id\", \"filename\"])\n",
    "\n",
    "# save as MANIFEST (comma-separated, no index; QIIME2 manifest is typically CSV)\n",
    "manifest_path = os.path.join(data_dir, \"MANIFEST\")\n",
    "manifest_df.to_csv(manifest_path, sep=\",\", index=False)\n",
    "\n",
    "print(f\"MANIFEST saved to: {manifest_path}\")\n",
    "print(manifest_df.head())\n",
    "print(manifest_df.columns)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Genome size distribution\n",
    "#    - We compute total sequence length (bp) for each MAG FASTA file,\n",
    "#      build a per-MAG DataFrame, and explore genome size statistics.\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "## Genome size distribution\n",
    "\n",
    "# Recursively list FASTA(-like) files under a root directory.\n",
    "def list_fasta_files(root):\n",
    "    root = Path(root)\n",
    "    exts = {\".fa\", \".fna\", \".fasta\"}\n",
    "    out = []\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file():\n",
    "            suf = p.suffix.lower()\n",
    "            # handle compressed filenames like .fa.gz\n",
    "            suf2 = \"\".join(p.suffixes[-2:]).lower()\n",
    "            if suf in exts or suf2 in {\".fa.gz\", \".fna.gz\", \".fasta.gz\"}:\n",
    "                out.append(p)\n",
    "    return out\n",
    "\n",
    "# Return total sequence length in bp for a FASTA file. Supports plain and gzipped.\n",
    "def fasta_total_length(path):\n",
    "    total = 0\n",
    "    open_fn = gzip.open if str(path).endswith(\".gz\") else open\n",
    "    # Use text mode with utf-8 fallback and ignore problematic characters\n",
    "    with open_fn(path, \"rt\", errors=\"ignore\") as fh:\n",
    "        for line in fh:\n",
    "            if not line or line.startswith(\">\"):\n",
    "                continue\n",
    "            total += len(line.strip())\n",
    "    return total\n",
    "\n",
    "# Build DataFrame with columns: mag_id, size_bp, _tech, path.\n",
    "def build_df_from_folder(folder, tech_label):\n",
    "    files = list_fasta_files(folder)\n",
    "    rows = []\n",
    "    for f in files:\n",
    "        # derive MAG ID from filename stem\n",
    "        mag_id = f.stem.replace(\".fa\",\"\").replace(\".fna\",\"\").replace(\".fasta\",\"\")\n",
    "        size_bp = fasta_total_length(f)\n",
    "        rows.append((mag_id, size_bp, tech_label, str(f)))\n",
    "    return pd.DataFrame(rows, columns=[\"mag_id\", \"size_bp\", \"_tech\", \"path\"])\n",
    "\n",
    "# Define the input folders for Illumina and PacBio MAGs\n",
    "illu_dir = Path(data_dir) / \"Illumina_MAGs\"\n",
    "pacb_dir = Path(data_dir) / \"PacBio_MAGs\"\n",
    "\n",
    "# 1) Build df_mag: concatenated genome-size table for both techniques\n",
    "#    Here we label Illumina as \"short\" and PacBio as \"long\".\n",
    "df_ill = build_df_from_folder(illu_dir, \"short\")  # Illumina = short-read\n",
    "df_pac = build_df_from_folder(pacb_dir, \"long\")   # PacBio  = long-read\n",
    "df_mag = pd.concat([df_ill, df_pac], ignore_index=True)\n",
    "\n",
    "# also compute genome size in Mbp for convenience / metadata export\n",
    "df_mag[\"_size_mbp\"] = df_mag[\"size_bp\"] / 1e6\n",
    "\n",
    "# Helper to print basic statistics for each technique\n",
    "def check_tech_label(df, tech_label):\n",
    "    print(f\"Checking {tech_label} data:\")\n",
    "    print(f\"  Max size: {df['size_bp'].max()} bp\")\n",
    "    print(f\"  Min size: {df['size_bp'].min()} bp\")\n",
    "    print(f\"  Mean size: {df['size_bp'].mean()} bp\")\n",
    "    print(f\"  Number of sequences: {len(df)}\")\n",
    "\n",
    "# Print Illumina and PacBio genome size statistics\n",
    "check_tech_label(df_ill, \"short\")  # Illumina\n",
    "check_tech_label(df_pac, \"long\")   # PacBio\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Build a QIIME 2–ready metadata TSV with genome size information\n",
    "#    - This table is later visualized with `qiime metadata tabulate`.\n",
    "# ---------------------------------------------------------------------\n",
    "meta = df_mag.rename(columns={\n",
    "    \"mag_id\":   \"sample-id\",\n",
    "    \"_tech\":    \"technique\",\n",
    "    \"_size_mbp\":\"genome_size_mbp\"\n",
    "})[[\"sample-id\", \"technique\", \"genome_size_mbp\", \"size_bp\", \"path\"]]\n",
    "\n",
    "meta.to_csv(f\"{data_dir}/genome_sizes_metadata.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"Wrote genome_sizes_metadata.tsv\")\n",
    "\n",
    "# Turn the metadata into an interactive QIIME 2 visualization\n",
    "! qiime metadata tabulate \\\n",
    "  --m-input-file $data_dir/genome_sizes_metadata.tsv \\\n",
    "  --o-visualization $data_dir/genome_sizes_table.qzv\n",
    "\n",
    "Visualization.load(f\"{data_dir}/genome_sizes_table.qzv\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Visual exploration of genome size distributions (Kbp, log scale)\n",
    "#    - Histogram, boxplot, and ECDF by sequencing technique.\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# --- Plot in Kbp ---\n",
    "\n",
    "# 0) Convert to Kbp\n",
    "df_k = df_mag.copy()\n",
    "df_k[\"_size_kbp\"] = df_k[\"size_bp\"] / 1e3  # size in Kbp\n",
    "\n",
    "# knobs to control plotting behavior\n",
    "LOG = True               # set True to use log-scale axes\n",
    "FOCUS_MAX_KBP = None     # set None for full range; 1000 Kbp = 1 Mbp\n",
    "\n",
    "# 1) Summary statistics per technique (useful for figure captions)\n",
    "summary = (\n",
    "    df_k.groupby(\"_tech\")[\"_size_kbp\"]\n",
    "        .agg(n=\"count\", mean=\"mean\", median=\"median\",\n",
    "             q25=lambda s: s.quantile(0.25),\n",
    "             q75=lambda s: s.quantile(0.75),\n",
    "             min=\"min\", max=\"max\")\n",
    "        .round(2)\n",
    ")\n",
    "print(summary)\n",
    "\n",
    "# 2) Histogram of genome sizes by technique\n",
    "plt.figure(figsize=(7, 4.5))\n",
    "x = df_k[\"_size_kbp\"].to_numpy()\n",
    "xmin = max(1, x[x>0].min()) if LOG else df_k[\"_size_kbp\"].min()\n",
    "xmax = df_k[\"_size_kbp\"].max()\n",
    "bins = (np.logspace(np.log10(xmin), np.log10(xmax), 35)\n",
    "        if LOG else np.linspace(xmin, np.quantile(x, 0.99), 35))\n",
    "\n",
    "for t in [\"short\", \"long\"]:\n",
    "    vals = df_k.loc[df_k[\"_tech\"] == t, \"_size_kbp\"].values\n",
    "    plt.hist(vals, bins=bins, alpha=0.6, label=t, density=True)\n",
    "\n",
    "if LOG:\n",
    "    plt.xscale(\"log\")\n",
    "if FOCUS_MAX_KBP is not None:\n",
    "    plt.xlim(0, FOCUS_MAX_KBP)\n",
    "plt.xlabel(\"Genome size (Kbp)\" + (\" [log]\" if LOG else \"\"))\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Genome size distribution by technique (Kbp)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Boxplot + jittered points by technique\n",
    "plt.figure(figsize=(6, 4.5))\n",
    "tech_order = [\"short\", \"long\"]\n",
    "data_for_box = [df_k.loc[df_k[\"_tech\"]==t, \"_size_kbp\"].values for t in tech_order]\n",
    "\n",
    "# Adjust boxplot appearance\n",
    "plt.boxplot(\n",
    "    data_for_box,\n",
    "    labels=tech_order,\n",
    "    showfliers=False,\n",
    "    widths=0.4,\n",
    "    notch=False,\n",
    "    patch_artist=False,  # no fill color\n",
    "    boxprops=dict(linewidth=1.5, color='black'),   # bold black border\n",
    "    medianprops=dict(linewidth=1.5, color='red'),  # red median line\n",
    "    whiskerprops=dict(linewidth=1.5, color='black')  # bold whiskers\n",
    ")\n",
    "\n",
    "# Add jittered individual points on top of the boxplot\n",
    "rng = np.random.default_rng(0)\n",
    "for xi, t in enumerate(tech_order, start=1):\n",
    "    vals = df_k.loc[df_k[\"_tech\"]==t, \"_size_kbp\"].values\n",
    "    jit = rng.normal(0, 0.04, size=len(vals))\n",
    "    plt.plot(np.full_like(vals, xi) + jit, vals, \"o\", ms=3, alpha=0.35)\n",
    "\n",
    "if LOG:\n",
    "    plt.yscale(\"log\")\n",
    "if FOCUS_MAX_KBP is not None:\n",
    "    plt.ylim(0, FOCUS_MAX_KBP)\n",
    "plt.ylabel(\"Genome size (Kbp)\" + (\" [log]\" if LOG else \"\"))\n",
    "plt.title(\"Genome size by technique (Kbp)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) ECDF (empirical cumulative distribution function) by technique\n",
    "def ecdf(x):\n",
    "    x = np.sort(x)\n",
    "    y = np.arange(1, len(x)+1) / len(x)\n",
    "    return x, y\n",
    "\n",
    "plt.figure(figsize=(7, 4.5))\n",
    "for t in tech_order:\n",
    "    x_ecdf, y_ecdf = ecdf(df_k.loc[df_k[\"_tech\"]==t, \"_size_kbp\"].values)\n",
    "    plt.step(x_ecdf, y_ecdf, where=\"post\", label=t)\n",
    "\n",
    "if LOG:\n",
    "    plt.xscale(\"log\")\n",
    "if FOCUS_MAX_KBP is not None:\n",
    "    plt.xlim(0, FOCUS_MAX_KBP)\n",
    "plt.xlabel(\"Genome size (Kbp)\" + (\" [log]\" if LOG else \"\"))\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.title(\"ECDF of genome sizes by technique (Kbp)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QIIME 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
