{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296fe46f-9449-404d-a618-d85f08341271",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook has been created as an addition to the main pipeline as is not necessarily part of the analysis process. Instead it takes the raw data provided and goes throught the process or shaping it to be ready to be imported into euler for the analytical pipeline. Chronologically, this notebook sits before ***01_Data_Exploration***. \n",
    "\n",
    "the files we recieved are:\n",
    "- *Illumina_MAGs.tar.gz*\n",
    "- *PacBio_MAGs.tar.gz*\n",
    "- *merged_metadata_filtered.tsv*\n",
    "\n",
    "The samples provided have been sequenced with each technique, meaning that intuitively all samples for a technique have a duplicate counterpart for the other. Feature files in *Illumina_MAGs* look to be already in [UUIDv4]('https://moshpit.qiime2.org/en/stable/chapters/howtos/import/') naming convention, but *PacBio_MAGs* are not.\n",
    "\n",
    "what is missing and the changes that need to be made are:\n",
    "- Rename features with the [UUIDv4]('https://moshpit.qiime2.org/en/stable/chapters/howtos/import/')\n",
    "- Create a primary key for all samples to retain technique information.\n",
    "- Update the metadata file to reflect the new primary key.\n",
    "- Create a metadata file feature-wide for preliminary analisis.\n",
    "- Fullfill qiime2 requirments for import\n",
    "    - Merge the 2 datasets in a sample_data common directory so that it is qiime-compatible.\n",
    "    - Create a MANIFEST file for **MAGS** in the sample_data directory.\n",
    "    - Clean sample_data of possible .tmp files that would interfere with qiime compatibility.\n",
    "- Compress sample_data to be exported to the Euler cluster for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192da93-c003-4662-ba9e-4d543e268b57",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48de6ba4-287c-40d3-ad81-a5db15b27132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: line 1: data_dir: command not found\n",
      "rm: cannot remove 'data/01_Data_Reshaping': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#set up environment\n",
    "import pandas as pd\n",
    "import qiime2 as q2\n",
    "from qiime2 import Visualization\n",
    "\n",
    "# create directories for the notebook. DO NOT change\n",
    "data_dir = 'data/01_Data_Reshaping'\n",
    "!data_dir = 'data/01_Data_Reshaping'\n",
    "\n",
    "# delete old folders\n",
    "!rm -r $data_dir\n",
    "\n",
    "\n",
    "!mkdir -p data\n",
    "!mkdir -p $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba38378-e6a5-4ac1-9382-f36ebe5184e4",
   "metadata": {},
   "source": [
    "# Downloading and unzipping raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db04ecac-86e2-4bd7-8464-84cdec643e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-07 17:01:40--  https://polybox.ethz.ch/index.php/s/56JaAiKdGwioBKN/download\n",
      "Resolving polybox.ethz.ch (polybox.ethz.ch)... 129.132.71.243\n",
      "Connecting to polybox.ethz.ch (polybox.ethz.ch)|129.132.71.243|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘additional_data/Download.zip’\n",
      "\n",
      "additional_data/Dow     [       <=>          ] 492.28M   392MB/s    in 1.3s    \n",
      "\n",
      "2025-12-07 17:01:41 (392 MB/s) - ‘additional_data/Download.zip’ saved [516188455]\n",
      "\n",
      "Archive:  additional_data/Download.zip\n",
      "   creating: data/01_Data_Reshaping/applied_bioinformatics/\n",
      " extracting: data/01_Data_Reshaping/applied_bioinformatics/.DS_Store  \n",
      " extracting: data/01_Data_Reshaping/applied_bioinformatics/Illumina_MAGs.tar.gz  \n",
      " extracting: data/01_Data_Reshaping/applied_bioinformatics/PacBio_MAGs.tar.gz  \n",
      " extracting: data/01_Data_Reshaping/applied_bioinformatics/merged_metadata_filtered.tsv  \n"
     ]
    }
   ],
   "source": [
    "# Download files from the polybox\n",
    "!wget 'https://polybox.ethz.ch/index.php/s/56JaAiKdGwioBKN/download'  -O additional_data/Download.zip\n",
    "# Unzip\n",
    "!unzip -o additional_data/Download.zip -d $data_dir\n",
    "!rm additional_data/Download.zip\n",
    "# extract contents from applied_bioinformatics folder\n",
    "!mv $data_dir/applied_bioinformatics/* $data_dir\n",
    "!rm -r $data_dir/applied_bioinformatics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596599c4-9a6a-42f7-bac2-84e0b44e4474",
   "metadata": {
    "tags": []
   },
   "source": [
    "## .tar.gz\n",
    "the MAGs are stored in .tar.gz files for Illumina sequencing and PacBio originating sequences. Extracting the files from the is the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7137901-123a-4a2f-b05d-81dc6145c263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#untar MAGs files and store everything in data_dir\n",
    "!tar -xzf $data_dir/Illumina_MAGs.tar.gz -C $data_dir\n",
    "!tar -xzf $data_dir/PacBio_MAGs.tar.gz -C $data_dir\n",
    "#remove the .tar.gz files\n",
    "!rm -r $data_dir/Illumina_MAGs.tar.gz\n",
    "!rm -r $data_dir/PacBio_MAGs.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f91c0-39db-4457-8183-d5ea9a92b677",
   "metadata": {},
   "source": [
    "<a id='UUID'></a>\n",
    "## UUIDv4 renaming\n",
    "a simple nested for loop that goes throught every file in the directories of interest and applies the uuid as the **new_path** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6cb744-022a-46e5-962f-2435557643d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "#Rename all fasta files with UUIDs\n",
    "for technique in os.listdir(data_dir):\n",
    "    tech_path = os.path.join(data_dir, technique)\n",
    "    if not os.path.isdir(tech_path):\n",
    "        continue\n",
    "    for sample_id in os.listdir(tech_path):\n",
    "        sample_path = os.path.join(tech_path, sample_id)\n",
    "        if not os.path.isdir(sample_path):\n",
    "            continue\n",
    "        for file in os.listdir(sample_path):\n",
    "            if file.endswith((\".fa\", \".fasta\")):\n",
    "                old_path = os.path.join(sample_path, file)\n",
    "                new_path = os.path.join(sample_path, f\"{uuid4()}.fa\")\n",
    "                os.rename(old_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6e0c9-bae2-4377-9aff-bb6860f04875",
   "metadata": {},
   "source": [
    "<a id='UUID'></a>\n",
    "\n",
    "## Primary Key Generation\n",
    "Using a for loop again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86fe0bb6-a40d-486a-8208-1d01a4bdd2e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$data_dir\"\n",
    "data_dir=\"$1\"\n",
    "# Rename all samples in Illumina_MAGs \n",
    "for d in $data_dir/Illumina_MAGs/*; do\n",
    "    mv \"$d\" \"$(dirname \"$d\")/IL_$(basename \"$d\")\"\n",
    "done\n",
    "for d in $data_dir/PacBio_MAGs/*; do\n",
    "    mv \"$d\" \"$(dirname \"$d\")/PB_$(basename \"$d\")\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a796e613-1e09-40fb-9ec8-8c676d97f801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import metadata\n",
    "metadata_df = pd.read_csv(f\"{data_dir}/merged_metadata_filtered.tsv\", sep=\"\\t\", index_col=0)\n",
    "\n",
    "#\n",
    "prefixes = (\"PB_\", \"IL_\")\n",
    "records = []\n",
    "\n",
    "for technique in os.listdir(data_dir):\n",
    "    tech_path = os.path.join(data_dir, technique)\n",
    "    if not os.path.isdir(tech_path):\n",
    "        continue\n",
    "    for sample_id in os.listdir(tech_path):\n",
    "        sample_path = os.path.join(tech_path, sample_id)\n",
    "        if not os.path.isdir(sample_path):\n",
    "            continue\n",
    "\n",
    "        # derive ID for lookup\n",
    "        lookup_id = sample_id\n",
    "        for p in prefixes:\n",
    "            if lookup_id.startswith(p):\n",
    "                lookup_id = lookup_id[len(p):]\n",
    "\n",
    "        for f in os.listdir(sample_path):\n",
    "            if f.endswith((\".fa\", \".fasta\")):\n",
    "                if lookup_id in metadata_df.index:\n",
    "                    mag_id = os.path.splitext(f)[0]\n",
    "                    records.append((sample_id, mag_id))\n",
    "\n",
    "primary_df = pd.DataFrame(records, columns=[\"sample-id\", \"mag-id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "991014ee-e276-43af-819d-47d6edf70ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PB_B039_Aa_Gp_La', 'PB_MS001-3', 'PB_9b8b5', 'PB_M008',\n",
       "       'PB_B038_Az_Gp_La', 'PB_e7c76', 'PB_P009', 'PB_M010',\n",
       "       'PB_HM010-03', 'PB_B056_Sc_Na_Af', 'PB_HM010-01', 'PB_M004',\n",
       "       'PB_36fe4', 'PB_a36ba', 'PB_M009', 'PB_P003', 'PB_P001', 'PB_M012',\n",
       "       'PB_A001', 'PB_MS013-1', 'PB_B051_Aj_Po_Laf', 'PB_MS003-2',\n",
       "       'PB_M006', 'PB_MS005-1', 'PB_B037_La_Ac_La', 'PB_MS009-1',\n",
       "       'PB_3ee22', 'IL_B038_Az_Gp_La', 'IL_A001', 'IL_P003', 'IL_MS003-3',\n",
       "       'IL_M009', 'IL_HM010-03', 'IL_M010', 'IL_B044_Hb_Ac_Ab',\n",
       "       'IL_MS009-2', 'IL_MS009-1', 'IL_MS001-3', 'IL_M002', 'IL_M012',\n",
       "       'IL_MS011-1', 'IL_B056_Sc_Na_Af', 'IL_M008', 'IL_B037_La_Ac_La',\n",
       "       'IL_MS013-1', 'IL_MS005-1', 'IL_MS003-2', 'IL_HM010-01', 'IL_P009',\n",
       "       'IL_B051_Aj_Po_Laf', 'IL_A002', 'IL_B039_Aa_Gp_La'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check sample-id\n",
    "primary_df['sample-id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c6d5b29-1447-43ee-a444-4accfed7ef7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample-id</th>\n",
       "      <th>mag-id</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PB_B039_Aa_Gp_La</td>\n",
       "      <td>829f489a-5001-40f7-b6f2-3cfca555ec09</td>\n",
       "      <td>B039_Aa_Gp_La</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PB_B039_Aa_Gp_La</td>\n",
       "      <td>559461fb-ff70-4610-9b15-fd7b67fcaefd</td>\n",
       "      <td>B039_Aa_Gp_La</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PB_B039_Aa_Gp_La</td>\n",
       "      <td>55e60f53-806c-486d-898f-f72cadfb37d6</td>\n",
       "      <td>B039_Aa_Gp_La</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sample-id                                mag-id         sample\n",
       "0  PB_B039_Aa_Gp_La  829f489a-5001-40f7-b6f2-3cfca555ec09  B039_Aa_Gp_La\n",
       "1  PB_B039_Aa_Gp_La  559461fb-ff70-4610-9b15-fd7b67fcaefd  B039_Aa_Gp_La\n",
       "2  PB_B039_Aa_Gp_La  55e60f53-806c-486d-898f-f72cadfb37d6  B039_Aa_Gp_La"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create 'sample' without the IL/PB prefix\n",
    "primary_df['sample'] = primary_df['sample-id'].str.split(pat='_', n=1).str[1]\n",
    "primary_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e088d7d9-89d1-4942-bffb-3098481b1e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samp_country</th>\n",
       "      <th>category</th>\n",
       "      <th>fermented_food_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MS009-1</th>\n",
       "      <td>Laos</td>\n",
       "      <td>fermented fish</td>\n",
       "      <td>Fermented_fish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A002</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>fermented meat</td>\n",
       "      <td>Fermented_pork_sausage_(Sai-krok_Isaan_Moo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3ee22</th>\n",
       "      <td>Germany</td>\n",
       "      <td>fermented vegetables</td>\n",
       "      <td>Sauerkraut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          samp_country              category  \\\n",
       "sample_ID                                      \n",
       "MS009-1           Laos        fermented fish   \n",
       "A002          Thailand        fermented meat   \n",
       "3ee22          Germany  fermented vegetables   \n",
       "\n",
       "                                   fermented_food_type  \n",
       "sample_ID                                               \n",
       "MS009-1                               Fermented_fish    \n",
       "A002       Fermented_pork_sausage_(Sai-krok_Isaan_Moo)  \n",
       "3ee22                                       Sauerkraut  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d115639-6bff-410a-af90-d284aac93e5b",
   "metadata": {},
   "source": [
    "## feature-based metadata\n",
    "for distributions and preliminary analisis a count of of fasta per could be useful. we merge primary_df with metadata on the left to achieve this result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e81474e-0513-423c-b1cf-53d7e08f16ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample-id</th>\n",
       "      <th>mag-id</th>\n",
       "      <th>sample</th>\n",
       "      <th>samp_country</th>\n",
       "      <th>category</th>\n",
       "      <th>fermented_food_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PB_B039_Aa_Gp_La</td>\n",
       "      <td>829f489a-5001-40f7-b6f2-3cfca555ec09</td>\n",
       "      <td>B039_Aa_Gp_La</td>\n",
       "      <td>Benin</td>\n",
       "      <td>fermented fish</td>\n",
       "      <td>Lanhouin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PB_B039_Aa_Gp_La</td>\n",
       "      <td>559461fb-ff70-4610-9b15-fd7b67fcaefd</td>\n",
       "      <td>B039_Aa_Gp_La</td>\n",
       "      <td>Benin</td>\n",
       "      <td>fermented fish</td>\n",
       "      <td>Lanhouin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PB_B039_Aa_Gp_La</td>\n",
       "      <td>55e60f53-806c-486d-898f-f72cadfb37d6</td>\n",
       "      <td>B039_Aa_Gp_La</td>\n",
       "      <td>Benin</td>\n",
       "      <td>fermented fish</td>\n",
       "      <td>Lanhouin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sample-id                                mag-id         sample  \\\n",
       "0  PB_B039_Aa_Gp_La  829f489a-5001-40f7-b6f2-3cfca555ec09  B039_Aa_Gp_La   \n",
       "1  PB_B039_Aa_Gp_La  559461fb-ff70-4610-9b15-fd7b67fcaefd  B039_Aa_Gp_La   \n",
       "2  PB_B039_Aa_Gp_La  55e60f53-806c-486d-898f-f72cadfb37d6  B039_Aa_Gp_La   \n",
       "\n",
       "  samp_country        category fermented_food_type  \n",
       "0        Benin  fermented fish            Lanhouin  \n",
       "1        Benin  fermented fish            Lanhouin  \n",
       "2        Benin  fermented fish            Lanhouin  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = primary_df.merge(metadata_df, left_on ='sample', right_index = True, how = 'left')\n",
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92a27e-6c56-4391-8b68-ff082251d083",
   "metadata": {},
   "source": [
    "The new *merged_df* can be saved as *Metadata_Extended.tsv* and used for exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09657ac-fc90-4c0c-aa82-577c588bc962",
   "metadata": {},
   "source": [
    "## Getting things ready for qiime2 Import\n",
    "### fixing directory tree\n",
    "To import sequences correctly into qiime we need to modify the directory tree to match from the current:\n",
    "\n",
    "data/  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;- technique 1  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;- technique2  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- sample 1  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- sample 2  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- seq1.fa  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- seq2.fa  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...  \n",
    "To the required:  \n",
    "data/  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;-seq1  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;-seq2  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;...  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;MANIFEST\n",
    "\n",
    "qiime would import the original tree without issue, but the subsequent BUSCO Evaluation job would fail. The error was due to some issue with the pathing. That is why the nesting was removed.\n",
    "\n",
    "Let's loop through all folders and move any .fa of .fasta files found to a new *data_dir/sample_data* folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35e758dd-938c-4f01-ba87-e2bae6b7464b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$data_dir\"\n",
    "data_dir=\"$1\"\n",
    "\n",
    "# Destination folder\n",
    "dest_dir=\"$data_dir/sample_data\"\n",
    "mkdir -p \"$dest_dir\"\n",
    "\n",
    "# Function to move fasta files safely\n",
    "move_fasta() {\n",
    "    folder=\"$1\"\n",
    "    # Use find to safely handle no matches\n",
    "    find \"$folder\" -type f \\( -name \"*.fa\" -o -name \"*.fasta\" \\) -exec mv {} \"$dest_dir\" \\;\n",
    "}\n",
    "\n",
    "# Move files from Illumina and PacBio folders\n",
    "move_fasta \"$data_dir/Illumina_MAGs\"\n",
    "move_fasta \"$data_dir/PacBio_MAGs\"\n",
    "\n",
    "# Delete the directory husk left behind\n",
    "rm -r $data_dir/Illumina_MAGs\n",
    "rm -r $data_dir/PacBio_MAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13afdc1f-de6a-4d10-ada7-3a0300530c13",
   "metadata": {},
   "source": [
    "### MANIFEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c7900a-6206-413c-bbad-df1b19f5acff",
   "metadata": {},
   "source": [
    "It is a file that associates each files data to their location. By following the [Moshpit Tutorial]('https://moshpit.qiime2.org/en/stable/chapters/howtos/import/') for importing non-dereplicated mags, no information about the MANIFEST format is provided. This is not a problem since the format was taken from the week 3 tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "098155cb-5d5d-4392-be6a-f7b7caa0972e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/01_Data_Reshaping/w3_MANIFEST'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#import MANIFEST from the tutorial\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m w3 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/w3_MANIFEST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m w3\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/01_Data_Reshaping/w3_MANIFEST'"
     ]
    }
   ],
   "source": [
    "#import MANIFEST from the tutorial\n",
    "w3 = pd.read_csv(f'{data_dir}/w3_MANIFEST', sep = '\\t')\n",
    "w3.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd4658c-120d-42e4-b953-0424f7926bc8",
   "metadata": {},
   "source": [
    "As you can be seen the MANIFEST needs to be a **2 column**, **tab separated** file to be compatible with qiime. From the Moshpit tutorial we know that the sequences files names have to be UUIDv4 generated. It should follow naturally that for the mags a **3 column, comma separated value** is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc684b9b-184a-48fc-897f-80975aac1131",
   "metadata": {
    "tags": []
   },
   "source": [
    "It is a matter of creating a df with **'sample-id'** and **'mags-id'** from our newly created *merged_df* and the **path** of each file in *sample_data*, making sure that the path matches the future location in the cluster, as they need to be imported in a cache for ease of access of the computational nodes. This has actually been one of the first roadblocks for the project as understanding how to get everything in ready-to-go state on Jupyter (where the UI is more visually intuitive for novice bioinformaticians) and move it to Euler was a real challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29519428-4e2f-4577-8e7a-ff3efe541eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Euler_dir = '/cluster/scratch/emotta/sample_data'  # target import directory\n",
    "\n",
    "# Build filename column from mag-id\n",
    "manifest_df = merged_df[[\"sample-id\", \"mag-id\"]].copy()\n",
    "manifest_df[\"filename\"] = manifest_df[\"mag-id\"].apply(\n",
    "    lambda x: f\"{Euler_dir}/{x}.fa\"\n",
    ")\n",
    "manifest_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2848fb-7e34-469b-80a7-d083e5a01848",
   "metadata": {},
   "source": [
    " This has easily been one of the most challenging steps in our project, since no proper documentation exist about this and even asking the the wednesday's session supervisors, the only guide was the error messages from the ***Mosh tools cache-import*** command when trying to import the sequences in qiime.\n",
    " \n",
    " <span style=\"color:red\">*MANIFEST is not a(n) MultiMAGManifestFormat file  \n",
    "Found header on line 1 with the following labels: ['sample-id\\tabsolute-filepath'], expected: ['sample-id', 'mag-id', 'filename']*</span>\n",
    "\n",
    "And later \n",
    "\n",
    "<span style=\"color:red\"> Found header on line 1 with the following labels: ['sample-id\\tmag-id\\tfilename'], expected: ['sample-id', 'mag-id', 'filename']</span>\n",
    "\n",
    "In retrospective it was not a complicated issue, but summed with the steep learning curve of both qiime and Euler simultaneously it added up to a great setback of the first part of the semester.\n",
    "\n",
    "We are now ready to save the MANIFEST in the *sample_data* directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c8e9e-c79a-43c2-9599-c4104378be2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "manifest_df.to_csv(f'{data_dir}/sample_data/MANIFEST', sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f114ae-b151-4d7d-82e6-49d0bf58986f",
   "metadata": {},
   "source": [
    "## Clean sample_data\n",
    "another common issue we encountered was the error warning was due to the presence of temporary and hidden files (*.files*) created by jupyter and moved from the raw data directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bcd57b-bcfa-469d-81f9-17482299e431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!find \"$data_dir/sample_data\" -maxdepth 1 -type f -name \".*\" -delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e183ed-73c8-46a2-b953-061dc23c3214",
   "metadata": {},
   "source": [
    "## Zip and upload to Euler\n",
    "*sample_data* has been "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fec51-6640-4683-aa72-84780d3759b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -czf sample_data.tar.gz -C \"$data_dir\" sample_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8dccfa-dafe-437a-945c-c505b507e569",
   "metadata": {},
   "source": [
    "**all set!** *sample_data* is now ready to be downloaded and manually uploaded to the ${HOME} directory on Euler for safe keeping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QIIME 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
